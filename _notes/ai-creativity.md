---
title: "The Last Test of AI"
date: 2026-01-12
permalink: /notes/ai-creativity/
author_profile: true
tags:
  - AI
  - creativity
  - writing
  - math
  - technology
---

keywords: blogs, AI, math, control systems

## AI测试的最后一波

2026.1.12号……AI测试的最后一波，之前一直想尝试，但是又没敢尝试的最后一波测试，怕被底裤脱的都不剩…今天还是试了。

我给了它一张复杂的控制框图，它**思考了2分30秒**，是近期单个任务中它思考的最久的。这不仅仅是一个答案的生成，而是一个深度的逻辑重构过程。在这段时间里，它成功推导出了**闭环输出导纳**的传递函数，而且全过程推导可追溯。

### 突破“历史学习”的局限

如果说双环控制下找输出导纳可能还有人做过，但是这张图中包含了一个特殊的 **$G_p$ 前馈补偿环（Prefilter）**。这个结构在我之前的论文里才出现过，按照大模型的原理，它本应是学习历史内容并找到可能性的答案，但它却能实时解析从未见过的拓扑逻辑。

#### 核心数学推导过程

根据来源，AI 首先对 Boost 变换器在 CCM 模式下进行了小信号建模。通过状态空间平均法（State-Space Averaging），它列出了电感电流与输出电压的平均方程：

$$\begin{cases}
L \frac{di_L}{dt} = v_g - (1-d)v_o \\
C \frac{dv_o}{dt} = (1-d)i_L - \frac{v_o}{R}
\end{cases}$$

在处理我给出的含 $G_p$ 的复杂双环结构时，它展现了极强的符号演算能力。它首先将左侧的局部回路等效化处理：

$$G_{cv,eq}(s) = \frac{G_{cv}(s)}{1 + G_{cv}(s)G_p(s)}$$

随后，它通过线性化电感方程和输出电流方程，利用拉普拉斯变换进行消元。最终推导出的**闭环输出导纳 $Y_T^{dbl}(s)$** 表达式如下：

$$Y_T^{dbl}(s) = sC + \frac{k^2 + k G_{ci} \bar{I}_L + k G_{ci} G_{cv,eq} \bar{V} - (sL) G_{ci} G_{cv,eq} \bar{I}_L}{sL + G_{ci} \bar{V}}$$

其中，$k = 1 - D$。**它不仅成功推出了传递函数，还精准识别了每一个物理项的来源**，例如 $-sC$ 支路对应电压变化通过电容电流对输出的影响，这是 Boost 变换器右半平面零点（RHP zero）的物理来源之一。

#### 表达式等价性的证明

更令人惊叹的是，当我将自己推导的完全展开形式与 AI 给出的结果进行对比时，AI 能够清晰地证明两者完全等价。

**AI 给出的紧凑形式**（使用 $G_{cv,eq}$）：

$$Y(s) = \frac{k^2 + kG_{ci}\bar{I}_L + kG_{ci}G_{cv,eq}\bar{V} - (sL)G_{ci}G_{cv,eq}\bar{I}_L}{sL + G_{ci}\bar{V}} + sC$$

其中 $G_{cv,eq} = \frac{G_{cv}}{1 + G_p G_{cv}}$。

**等价性证明过程**：

AI 指出，将 $G_{cv,eq}$ 代入后，为了消除分式，整体乘以 $\frac{1 + G_p G_{cv}}{1 + G_p G_{cv}}$，得到：

- **分母变成**：$(sL + G_{ci}\bar{V})(1 + G_p G_{cv})$（这正是我推导结果中的 $D(s)$）

- **分子变成**：$k^2(1 + G_p G_{cv}) + kG_{ci}\bar{I}_L(1 + G_p G_{cv}) + kG_{ci}G_{cv}\bar{V} - (sL)G_{ci}G_{cv}\bar{I}_L$

将 $k = 1 - D$ 换回去，就和我写的 $N_1(s)$ 逐项一致。

**结论**：
- 我的分母 $D(s)$ 正是 $(sL + G_{ci}\bar{V})(1 + G_p G_{cv})$
- 我的第二个分式严格约掉就是 $+sC$
- 我的第一分式的分子，等价于把 AI 那条式子里 $G_{cv,eq} = \frac{G_{cv}}{1 + G_p G_{cv}}$ 展开后通分得到的结果

**所以：完全一样，只是写法不同。**

这种能够识别不同数学表达形式之间的等价关系，并进行严格证明的能力，已经超出了简单的"学习历史"范畴。

### "自我验证"的逻辑震撼

最让我震撼的是，它能将**前馈结构 $G_p$ 趋近于 0 来自我验证**。它指出，当 $G_p = 0$ 时，$G_{cv,eq}$ 退化回 $G_{cv}$，整个复杂的导纳公式随之退化为标准的双环输出导纳模型。**这跟我当年的思路一模一样**。

至此，大厦全部坍塌。身上能想到的所有技能都在过去的几个月里让AI都走了一遍，感觉它做的都比我好。不是说大模型的原理只是学习历史吗？我怎么感觉不仅仅是这样。貌似大家都不谈能不能通过图灵测试了，貌似大家的集体潜意识都默认它通过了……

这下真成**工业革命的弃子**了……真叫人绝望。
**凡有所相，皆为虚妄。**
AI一天，人间十年……
